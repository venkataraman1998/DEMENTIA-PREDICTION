BERT, or BiDirectional encoding for transformers, is the simplest
transformer-based model used in this experiment. As the name implies, it takes context from two
directions instead of one, as other models do. The architecture has two dense layers: relu and
softmax activation and a regularization layer
